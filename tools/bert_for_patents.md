---
authors: Google Patents, Rob Srebrovic, Jay Yonamine
citation: https://cloud.google.com/blog/products/ai-machine-learning/how-ai-improves-patent-analysis
description: A BERT (bidirectional encoder representation from transformers) model
  pretrained on over 100 million patent publications from the U.S. and other countries
  using open-source tooling. The trained model can be used for a number of use cases,
  including how to more effectively perform prior art searching to determine the novelty
  of a patent application, automatically generate classification codes to assist with
  patent categorization, and autocomplete.
documentation: https://github.com/google/patents-public-data/blob/master/examples/BERT_For_Patents.ipynb
last_edit: Fri, 01 Dec 2023 12:20:34 GMT
location: https://github.com/google/patents-public-data/blob/master/models/BERT%20for%20Patents.md
related_projects: {}
shortname: bert_for_patents
tags:
- classification
- novelty
- machine learning
terms_of_use: http://www.apache.org/licenses/LICENSE-2.0
title: Google BERT for Patents
uuid: b8c70382-7b6f-43b2-a6c0-c788e970e99e
---